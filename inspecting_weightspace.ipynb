{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "fRMRVCQPXAWM"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcRL2FwjtSZ3"
      },
      "outputs": [],
      "source": [
        "!wget -nc https://osf.io/8qcdp/download -O ObjectOrientationData.mat\n",
        "!wget -nc https://osf.io/49qeb/download -O InanimateObjectsData.mat\n",
        "!wget -nc https://osf.io/x9dz4/download -O InanimateObjects.zip\n",
        "!unzip InanimateObjects.zip\n",
        "!mkdir -p Stimuli \n",
        "!mv InanimateObjects ./Stimuli/InanimateObjects\n",
        "!wget -c https://raw.githubusercontent.com/harvard-visionlab/sroh/main/2022/feature_extractor.py\n",
        "\n",
        "!wget -nc https://www.dropbox.com/s/hq3n4ti12t9mm7g/ExampleRDMS_EarlyV_conv_block_2.1.pth.tar?dl=0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting Brain Data\n"
      ],
      "metadata": {
        "id": "3dYKnEK0ofw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import scipy.io as sio\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "def load_brain_data(dataset, brain_regions=['EarlyV', 'pOTC', 'aOTC']):\n",
        "    assert dataset in ['InanimateObjects', 'ObjectOrientation']\n",
        "    path = f'{dataset}Data.mat'\n",
        "    D = sio.loadmat(path, struct_as_record=False, squeeze_me=True)\n",
        "    rdms = {r: D['rdms'].__dict__[r] for r in brain_regions}\n",
        "    betas = {r: D['betas'].__dict__[r] for r in brain_regions}\n",
        "    reliability = {r: D['reliability'].__dict__[r] for r in brain_regions}\n",
        "    image_names = [f.strip() for f in D['image_names']]\n",
        "    return rdms, betas, reliability, image_names"
      ],
      "metadata": {
        "id": "xBhccI8UtlbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdms, betas, reliability, image_names = load_brain_data('InanimateObjects')"
      ],
      "metadata": {
        "id": "KvjIV8wBt_IU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RidgeCV"
      ],
      "metadata": {
        "id": "bJalrH1KooPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from fastprogress.fastprogress import progress_bar\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Ridge, RidgeCV\n",
        "from sklearn.metrics import r2_score\n",
        "from pdb import set_trace\n",
        "from copy import deepcopy\n",
        "from tqdm import tqdm\n",
        "import sklearn\n",
        "\n",
        "default_alphas = np.concatenate([np.array([1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1.0]), np.logspace(1, 5, 50)])\n",
        "\n",
        "def leave_one_out_ridge(X, y, alphas=default_alphas, fit_intercept=True, normalize=True, mb=None):\n",
        "    '''\n",
        "        Construct predicted brain patterns by training on N-1 items, \n",
        "        and then predicting the held out item.\n",
        "        \n",
        "        X: model responses [numItems x numFeatures]\n",
        "        y: brain responses [numItems x numVoxels]\n",
        "    '''\n",
        "    n_items, n_features = X.shape\n",
        "    n_voxels = y.shape[1]\n",
        "    y_pred = np.zeros(y.shape)\n",
        "\n",
        "    ALPHAS = []\n",
        "    COEF_M = np.zeros((n_voxels, n_features))\n",
        "    INTERCEPT = []\n",
        "    for iter_count, test_idx in enumerate(progress_bar(range(n_items), total=n_items)):\n",
        "        train_idxs = np.ones(n_items) == True\n",
        "        train_idxs[test_idx] = False\n",
        "        test_idxs = ~train_idxs  \n",
        "\n",
        "        clf = RidgeCV(alphas=alphas, fit_intercept=fit_intercept)\n",
        "\n",
        "        if normalize:\n",
        "            scaler = StandardScaler()\n",
        "            scaler.fit(X[train_idxs])\n",
        "            X_train = scaler.transform(X[train_idxs])\n",
        "            X_test = scaler.transform(X[test_idxs])        \n",
        "        else:\n",
        "            X_train = X[train_idxs]\n",
        "            X_test = X[test_idxs]\n",
        "              \n",
        "        clf.fit(X_train, y[train_idxs])\n",
        "        y_pred[test_idxs] = clf.predict(X_test)\n",
        "        \n",
        "        ALPHAS.append(clf.alpha_)\n",
        "        COEF_M += clf.coef_\n",
        "        INTERCEPT.append(clf.intercept_)\n",
        "\n",
        "    ALPHAS = np.stack(ALPHAS)\n",
        "    COEF_M /= iter_count\n",
        "    INTERCEPT = np.stack(INTERCEPT)\n",
        "    R2 = r2_score(y, y_pred, multioutput='raw_values')\n",
        "    \n",
        "    return {\n",
        "        \"n_items\": n_items,\n",
        "        \"n_features\": n_features,\n",
        "        \"n_voxels\": y.shape[1],\n",
        "        \"ALPHAS\": ALPHAS,\n",
        "        \"COEF_M\": COEF_M,\n",
        "        \"INTERCEPT\": INTERCEPT,\n",
        "        \"R2\": R2,\n",
        "        \"y_pred\": y_pred\n",
        "    }"
      ],
      "metadata": {
        "id": "SEjVyNxu2Kam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import models, transforms \n",
        "from PIL import Image \n",
        "from natsort import natsorted \n",
        "from glob import glob \n",
        "from pathlib import Path \n",
        "from feature_extractor import FeatureExtractor\n",
        "\n",
        "def prepare_images(dataset='InanimateObjects', mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
        "\n",
        "    # standard imagenet normalization\n",
        "    normalize = transforms.Normalize(mean=mean, std=std)\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        lambda x: Image.open(x),    # use PIL to open the image\n",
        "        transforms.Resize(224),     # resize shorted edge to 224 pixels\n",
        "        transforms.CenterCrop(224), # center crop if not square\n",
        "        transforms.ToTensor(),      # convert from RGB (HxWxC) to channels first torch tensor [CxHxW]\n",
        "        normalize                   # normalize by imagenet stats\n",
        "    ])\n",
        "    files = natsorted(glob(f'./Stimuli/{dataset}/*.jpg'))\n",
        "    file_names = [Path(f).name for f in files] \n",
        "    imgs = torch.stack([transform(f) for f in files])\n",
        "\n",
        "    return imgs\n",
        "\n",
        "def fit_encoding_model(betas, layer_name, model_name='alexnet',\n",
        "                       dataset='InanimateObjects', mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
        "          \n",
        "    print(\"==> prepare images\")\n",
        "    imgs = prepare_images(dataset=dataset, mean=mean, std=std)\n",
        "    \n",
        "    print(\"==> load pretrained model\")\n",
        "    model = models.__dict__[model_name](pretrained=True)\n",
        "\n",
        "    print(\"==> extract activation map for the given layer\")\n",
        "    pred_rdms = {}\n",
        "    feat_rdms = {}\n",
        "    model.eval()   # <-- very important, freeze normalization stats, no dropout etc.\n",
        "    with FeatureExtractor(model, [layer_name]) as extractor:\n",
        "        features = extractor(imgs)\n",
        "        for layer_name,feat in features.items():\n",
        "            # retain spatial information, but flatten rows into a 1D feature vector\n",
        "            X = torch.flatten(feat, 1)\n",
        "            feat_rdm = 1 - np.corrcoef(X)\n",
        "            feat_rdms[layer_name] = feat_rdm\n",
        "            \n",
        "            print(f\"==> fitting ridge regression model ({layer_name}) (numFeatures={X.shape[1]})\")\n",
        "            results = leave_one_out_ridge(X, betas, fit_intercept=True, normalize=False)\n",
        "            \n",
        "            # compute the predicted neural RDM\n",
        "            pred_rdm = 1 - np.corrcoef(results['y_pred'])\n",
        "            pred_rdms[layer_name] = pred_rdm\n",
        "              \n",
        "            # now do something with the rdms, e.g., save them for our split-half analysis\n",
        "          \n",
        "    return pred_rdms, feat_rdms, results"
      ],
      "metadata": {
        "id": "morbezfKue3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdms.keys()"
      ],
      "metadata": {
        "id": "GmGciH1333Z1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdms['EarlyV'].shape"
      ],
      "metadata": {
        "id": "ZMWLfwwO-M9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sub_betas = betas['EarlyV'][0].transpose()\n",
        "sub_betas.shape"
      ],
      "metadata": {
        "id": "2At_mQhd5hgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.alexnet(pretrained=True)"
      ],
      "metadata": {
        "id": "BgKLpgMBLNVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_rdms, feat_rdms, results = fit_encoding_model(sub_betas, \n",
        "                                                   model_name='alexnet', \n",
        "                                                   layer_name='classifier.1',\n",
        "                                                   dataset='InanimateObjects', \n",
        "                                                   mean=[0.485, 0.456, 0.406], \n",
        "                                                   std=[0.229, 0.224, 0.225])"
      ],
      "metadata": {
        "id": "o7NLl51u5k4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(np.abs(np.mean(coef_m, axis=0)))"
      ],
      "metadata": {
        "id": "jRa3OZ-QAw8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "coef_m = results['COEF_M']\n",
        "#abs_coef_m = np.abs(coef_m)\n",
        "x = np.abs(np.mean(abs_coef_m, axis=0))\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.xlabel('weight values', size=15)\n",
        "plt.ylabel('frequency of features', size=15)\n",
        "plt.hist(np.abs(np.mean(coef_m, axis=0)), color='darkorange', alpha=.5)\n",
        "plt.savefig('ex_weights.png', dpi=300)"
      ],
      "metadata": {
        "id": "QxUqcnhP_41i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#should chagne this - make keys the layers - and should aleady be subx72x72 array\n",
        "print(\"pred_rdms.keys():\", pred_rdms.keys())"
      ],
      "metadata": {
        "id": "bdD9ATkh8Y55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from load_brain_data - single sub\n",
        "print(\"rdms.keys:\", rdms.keys())\n",
        "rdms['EarlyV'][1].shape"
      ],
      "metadata": {
        "id": "KTJ9340zHowL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from fit_encoding_model - single sub\n",
        "print(\"pred_rdm.keys:\", pred_rdms.keys())\n",
        "pred_rdms['classifier.5'].shape"
      ],
      "metadata": {
        "id": "Z6EoCU31Hw0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from fit_encoding_model - single sub\n",
        "print(\"feat_rdm.keys:\", feat_rdms.keys())\n",
        "feat_rdms['classifier.5'].shape"
      ],
      "metadata": {
        "id": "-8AAEr3ZHx4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 1: Outputs like Paper?"
      ],
      "metadata": {
        "id": "RO0rOPDlBWsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#group-half RSA\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from itertools import combinations\n",
        "from copy import deepcopy\n",
        "from scipy.stats import pearsonr\n",
        "from fastprogress.fastprogress import progress_bar\n",
        "\n",
        "def get_split_halves(N):\n",
        "    subjects = list(range(0,N))\n",
        "    splits = []\n",
        "    count = 0\n",
        "    for subsetA in combinations(subjects, N//2):\n",
        "        subsetA = list(subsetA)\n",
        "        subsetB = list(np.setdiff1d(subjects, subsetA))\n",
        "        assert len(np.setdiff1d(subsetA,subsetB)) == len(subsetA), \"oops\"\n",
        "        assert len(np.setdiff1d(subsetB,subsetA)) == len(subsetB), \"oops\"\n",
        "        assert (len(subsetA) + len(subsetB)) == N, f\"oops, total should be {N}\"\n",
        "        splits.append((subsetA,subsetB))\n",
        "        count+=1\n",
        "    \n",
        "    return splits[0:len(splits)//2] if N%2==0 else splits\n",
        "\n",
        "def compute_avg_rdm(rdms):\n",
        "        \n",
        "    # convert dissimilarity matrix back to similarity matix (-1.0 to 1.0)\n",
        "    RSMS = 1 - rdms\n",
        "\n",
        "    # fisherz transform for averaging (gives a group similarity matrix, values still z-transformed)\n",
        "    group_zrsm = fisherz(RSMS).mean(axis=0, keepdims=True)\n",
        "\n",
        "    # compute group rdm (1 - fisherz_inv(ZRSM))\n",
        "    avg_rdm = 1 - fisherz_inv(group_zrsm)\n",
        "    \n",
        "    return avg_rdm\n",
        "\n",
        "def get_rdm_subset(rdms, brain_region, idxs):\n",
        "    rdms = deepcopy(rdms)\n",
        "    return rdms[brain_region][idxs]\n",
        "\n",
        "def fisherz(r, eps=1e-5):\n",
        "    return np.arctanh(r-eps)\n",
        "\n",
        "def fisherz_inv(z):\n",
        "    return np.tanh(z)\n",
        "\n",
        "def compute_adjusted_ci(scores, CI=1.96):\n",
        "    scores = np.array(scores)\n",
        "    N = len(scores)\n",
        "    mean = scores.mean()\n",
        "    var = scores.var(ddof=1)    \n",
        "\n",
        "    # sem = sd / np.sqrt(N)\n",
        "    # to adjust for non-indepndence of split halves, divide by np.sqrt( (N + n2/n1)/N )\n",
        "    # where n1 = number of train samples, n2 = number of test samples, in our case\n",
        "    # n1 = 1 (the single group TrainRDM), and n2 = 1 (the single TestRDM)\n",
        "    sem = np.sqrt( (1/N+1/1) * var)\n",
        "    ci = sem * CI\n",
        "    lower = mean - ci\n",
        "    upper = mean + ci\n",
        "\n",
        "    return mean, lower, upper\n",
        "\n",
        "def compute_fisherz_ci(scores):\n",
        "    scores = np.array(scores)\n",
        "    out = compute_adjusted_ci(fisherz(scores))\n",
        "    return [fisherz_inv(o) for o in out]\n",
        "\n",
        "def compare_rdms(target_rdm, model_rdm):\n",
        "    assert target_rdm.shape == model_rdm.shape, \"oops, rdms must have same shape!\"\n",
        "    \n",
        "    target = np.array(target_rdm[np.triu_indices(target_rdm.shape[0], k=1)])\n",
        "    predicted = np.array(model_rdm[np.triu_indices(model_rdm.shape[0], k=1)])\n",
        "\n",
        "    return pearsonr(target, predicted)[0]\n",
        "\n",
        "def update_df(df, brain_region, model_name, layer_name, split_num, split_idxs, group, sim):\n",
        "    \n",
        "    df = df.append({\n",
        "        \"analysis\": \"group_halves_rsa\",\n",
        "        \"model_name\": model_name,\n",
        "        \"dataset\": \"InanimateObjects\",\n",
        "        \"brain_region\": brain_region,\n",
        "        \"layer_name\": layer_name,\n",
        "        \"split_num\": split_num,\n",
        "        \"split_idxs\": str(split_idxs),\n",
        "        \"group\": group,        \n",
        "        \"pearsonr\": sim,\n",
        "    }, ignore_index=True)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "LaL3wsGPBbRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sample Data"
      ],
      "metadata": {
        "id": "gO4A6aOzeHPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#sample data\n",
        "data = torch.load('ExampleRDMS_EarlyV_conv_block_2.1.pth.tar?dl=0')\n",
        "print(data.keys())\n",
        "brain_region = data['brain_region']\n",
        "layer_name = data['layer_name']\n",
        "print(\"brain region:\", brain_region)\n",
        "print(\"layer name:\", layer_name)\n",
        "sample_neural_rdms = data['true_rdms']\n",
        "sample_pred_rdms = data['pred_rdms']\n",
        "sample_feat_rdms = data['feat_rdms']\n",
        "sample_neural_rdms.shape\n",
        "sample_pred_rdms.shape\n",
        "sample_feat_rdms.shape\n",
        "#10,72,72"
      ],
      "metadata": {
        "id": "NvpDyPHQ13tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sample data\n",
        "N = sample_pred_rdms.shape[0]\n",
        "splits = get_split_halves(N)\n",
        "print(f\"N={N}, num_splits={len(splits)}\")"
      ],
      "metadata": {
        "id": "K8UWZSA4mglL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sample_data - pred to neural\n",
        "model_name = \"ipcl_alexnet_gn\"\n",
        "df = pd.DataFrame(columns=['analysis','model_name','dataset','brain_region',\n",
        "                           'layer_name', 'split_num', 'split_idxs', 'group', 'pearsonr'])\n",
        "\n",
        "for split_num, (group1, group2) in enumerate(progress_bar(splits)):\n",
        "    idx_group1 = np.array(group1)\n",
        "    idx_group2 = np.array(group2)                                \n",
        "\n",
        "    brain_rdm1 = compute_avg_rdm(sample_neural_rdms[idx_group1]).squeeze()\n",
        "    brain_rdm2 = compute_avg_rdm(sample_neural_rdms[idx_group2]).squeeze()\n",
        "    \n",
        "    pred_rdm1 = compute_avg_rdm(sample_pred_rdms[idx_group1]).squeeze()\n",
        "    pred_rdm2 = compute_avg_rdm(sample_pred_rdms[idx_group2]).squeeze()                                \n",
        "\n",
        "    sim1 = compare_rdms(brain_rdm1, pred_rdm1)\n",
        "    sim2 = compare_rdms(brain_rdm2, pred_rdm2)\n",
        "\n",
        "    # record the results\n",
        "    df = update_df(df, brain_region, model_name, layer_name, split_num, \n",
        "                   idx_group1, 'group1', sim1)\n",
        "\n",
        "    df = update_df(df, brain_region, model_name, layer_name, split_num, \n",
        "                   idx_group2, 'group2', sim2)\n",
        "df"
      ],
      "metadata": {
        "id": "WXjDzayRmo7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sample_data - pred to neural\n",
        "mean, lower, upper = compute_fisherz_ci(df.pearsonr)\n",
        "print(f\"mean={mean:3.3f}, 95% CI=[{lower:3.3f},{upper:3.3f}]\")"
      ],
      "metadata": {
        "id": "KB3m79Zlnbbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sample_data - feat to neural\n",
        "model_name = \"ipcl_alexnet_gn\"\n",
        "df = pd.DataFrame(columns=['analysis','model_name','dataset','brain_region',\n",
        "                           'layer_name', 'split_num', 'split_idxs', 'group', 'pearsonr'])\n",
        "\n",
        "for split_num, (group1, group2) in enumerate(progress_bar(splits)):\n",
        "    idx_group1 = np.array(group1)\n",
        "    idx_group2 = np.array(group2)                                \n",
        "\n",
        "    brain_rdm1 = compute_avg_rdm(sample_neural_rdms[idx_group1]).squeeze()\n",
        "    brain_rdm2 = compute_avg_rdm(sample_neural_rdms[idx_group2]).squeeze()\n",
        "    \n",
        "    feat_rdm1 = compute_avg_rdm(sample_feat_rdms[idx_group1]).squeeze()\n",
        "    feat_rdm2 = compute_avg_rdm(sample_feat_rdms[idx_group2]).squeeze()                                \n",
        "\n",
        "    sim1 = compare_rdms(brain_rdm1, feat_rdm1)\n",
        "    sim2 = compare_rdms(brain_rdm2, feat_rdm2)\n",
        "\n",
        "    # record the results\n",
        "    df = update_df(df, brain_region, model_name, layer_name, split_num, \n",
        "                   idx_group1, 'group1', sim1)\n",
        "\n",
        "    df = update_df(df, brain_region, model_name, layer_name, split_num, \n",
        "                   idx_group2, 'group2', sim2)\n",
        "df"
      ],
      "metadata": {
        "id": "1__srj9OoqmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sample_data - feat to neural\n",
        "mean, lower, upper = compute_fisherz_ci(df.pearsonr)\n",
        "print(f\"mean={mean:3.3f}, 95% CI=[{lower:3.3f},{upper:3.3f}]\")"
      ],
      "metadata": {
        "id": "x_6zOp8Aovl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "My Data"
      ],
      "metadata": {
        "id": "4Np74yuNf8dq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#can do fit encoding model in this for loop - and dont have to save dicts\n",
        "subs = [0,1,2,3,4,5,6,7,8,9]\n",
        "layer_name='classifier.5'\n",
        "for sub in subs:\n",
        "  print(\"\\n\\nSUB:\", sub)\n",
        "  sub_betas = betas['EarlyV'][sub].transpose()\n",
        "  pred_rdms, feat_rdms, results = fit_encoding_model(sub_betas, \n",
        "                                                   model_name='alexnet', \n",
        "                                                   layer_name='classifier.5',\n",
        "                                                   dataset='InanimateObjects', \n",
        "                                                   mean=[0.485, 0.456, 0.406], \n",
        "                                                   std=[0.229, 0.224, 0.225])\n",
        "  \n",
        "  pred_rdm = pred_rdms[layer_name].reshape((1,72,72))\n",
        "  feat_rdm = feat_rdms[layer_name].reshape((1,72,72))\n",
        "  if sub == 0:\n",
        "    pred_rdms1 = pred_rdm   #pred_rdms1 so that pred_rdms doesnt get relabeled!\n",
        "    feat_rdms1 = feat_rdm   #feat_rdm1 so that feat_rdms doesnt get relabeled!\n",
        "  else:\n",
        "    pred_rdms1 = np.concatenate((pred_rdms1, pred_rdm), axis=0)\n",
        "    feat_rdms1 = np.concatenate((feat_rdms1, feat_rdm), axis=0)\n",
        "\n"
      ],
      "metadata": {
        "id": "aFQKL5vI6dPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_rdms1.shape\n",
        "feat_rdms1.shape\n",
        "rdms['EarlyV'].shape\n",
        "#10,72,72 - 10 subs, each has 72x72 rdm (num items)"
      ],
      "metadata": {
        "id": "-_sqYjJljf3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#neural and model rdms - no lesions\n",
        "brain_region = 'EarlyV'\n",
        "layer_name = \"classifier.5\"\n",
        "neural_rdms = rdms[brain_region]\n",
        "pred_rdms1 #pred_rdms1 so that pred_rdms doesnt get relabeled!\n",
        "feat_rdms1 #feat_rdm1 so that feat_rdms doesnt get relabeled!\n",
        "assert pred_rdms1.shape == neural_rdms.shape, \"oops, expected same shapes\""
      ],
      "metadata": {
        "id": "vRsmy-qe-AZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# iterate over all possible splits (for 10 subjects, 126 unique splits of the data, 252 unique groups total)\n",
        "N = pred_rdms1.shape[0]\n",
        "splits = get_split_halves(N)\n",
        "print(f\"N={N}, num_splits={len(splits)}\")"
      ],
      "metadata": {
        "id": "THvl6Vt9Enun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pred1 to neural\n",
        "df = pd.DataFrame(columns=['analysis','model_name','dataset','brain_region',\n",
        "                           'layer_name', 'split_num', 'split_idxs', 'group', 'pearsonr'])\n",
        "\n",
        "model_name = 'alexnet'\n",
        "for split_num, (group1, group2) in enumerate(progress_bar(splits)):\n",
        "    idx_group1 = np.array(group1)\n",
        "    idx_group2 = np.array(group2)                                \n",
        "\n",
        "    brain_rdm1 = compute_avg_rdm(neural_rdms[idx_group1]).squeeze()\n",
        "    brain_rdm2 = compute_avg_rdm(neural_rdms[idx_group2]).squeeze()\n",
        "    \n",
        "    pred_rdm1 = compute_avg_rdm(pred_rdms1[idx_group1]).squeeze()\n",
        "    pred_rdm2 = compute_avg_rdm(pred_rdms1[idx_group2]).squeeze()                                \n",
        "\n",
        "    sim1 = compare_rdms(brain_rdm1, pred_rdm1)\n",
        "    sim2 = compare_rdms(brain_rdm2, pred_rdm2)\n",
        "\n",
        "    # record the results\n",
        "    df = update_df(df, brain_region, model_name, layer_name, split_num, \n",
        "                   idx_group1, 'group1', sim1)\n",
        "\n",
        "    df = update_df(df, brain_region, model_name, layer_name, split_num, \n",
        "                   idx_group2, 'group2', sim2)\n",
        "df"
      ],
      "metadata": {
        "id": "9F2qMW1PGh3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pred1 to neural\n",
        "mean, lower, upper = compute_fisherz_ci(df.pearsonr)\n",
        "print(f\"mean={mean:3.3f}, 95% CI=[{lower:3.3f},{upper:3.3f}]\")"
      ],
      "metadata": {
        "id": "-TCTptvvA_mU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#feat1 to neural\n",
        "df = pd.DataFrame(columns=['analysis','model_name','dataset','brain_region',\n",
        "                           'layer_name', 'split_num', 'split_idxs', 'group', 'pearsonr'])\n",
        "\n",
        "model_name = 'alexnet'\n",
        "for split_num, (group1, group2) in enumerate(progress_bar(splits)):\n",
        "    idx_group1 = np.array(group1)\n",
        "    idx_group2 = np.array(group2)                                \n",
        "\n",
        "    brain_rdm1 = compute_avg_rdm(neural_rdms[idx_group1]).squeeze()\n",
        "    brain_rdm2 = compute_avg_rdm(neural_rdms[idx_group2]).squeeze()\n",
        "    \n",
        "    feat_rdm1 = compute_avg_rdm(feat_rdms1[idx_group1]).squeeze()\n",
        "    feat_rdm2 = compute_avg_rdm(feat_rdms1[idx_group2]).squeeze()                                \n",
        "\n",
        "    sim1 = compare_rdms(brain_rdm1, feat_rdm1)\n",
        "    sim2 = compare_rdms(brain_rdm2, feat_rdm2)\n",
        "\n",
        "    # record the results\n",
        "    df = update_df(df, brain_region, model_name, layer_name, split_num, \n",
        "                   idx_group1, 'group1', sim1)\n",
        "\n",
        "    df = update_df(df, brain_region, model_name, layer_name, split_num, \n",
        "                   idx_group2, 'group2', sim2)\n",
        "df"
      ],
      "metadata": {
        "id": "ojtkdjTbJiDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#feat1 to neural\n",
        "mean, lower, upper = compute_fisherz_ci(df.pearsonr)\n",
        "print(f\"mean={mean:3.3f}, 95% CI=[{lower:3.3f},{upper:3.3f}]\")"
      ],
      "metadata": {
        "id": "uFZClTW9Jwfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save and dowloading df as csv\n",
        "df.to_csv(brain_region+\"_\"+model_name+\"_\"+layer_name+\"_df\")\n",
        "#from google.colab import files\n",
        "#files.download(brain_region+\"_\"+model_name+\"_\"+layer_name)"
      ],
      "metadata": {
        "id": "ZhUpDfC_IDIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Cherry Pick a well Predicted Voxel"
      ],
      "metadata": {
        "id": "nSQ9oyuDsZ-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#dims of coef_m\n",
        "sub_betas = betas['EarlyV'][1].transpose()\n",
        "pred_rdms, feat_rdms, results = fit_encoding_model(sub_betas, \n",
        "                                                   model_name='alexnet', \n",
        "                                                   layer_name='classifier.5',\n",
        "                                                   dataset='InanimateObjects', \n",
        "                                                   mean=[0.485, 0.456, 0.406], \n",
        "                                                   std=[0.229, 0.224, 0.225])\n",
        "results['n_voxels'], results['n_features']\n",
        "results['COEF_M'].shape"
      ],
      "metadata": {
        "id": "a-bNyKh75gXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get n number of well predicted voxels - list format\n",
        "def get_n_well_predicted_voxs(r2, num_vox):\n",
        "  vox_idxs = []\n",
        "  for i in range(1, num_vox+1):  \n",
        "    r2_copy = r2.copy()\n",
        "    r2_sorted = np.sort(r2_copy)\n",
        "    max_vox_loc = np.argwhere(r2 == r2_sorted[-i])\n",
        "    max_vox = max_vox_loc[0][0]\n",
        "    vox_idxs.append(max_vox)\n",
        "  return vox_idxs"
      ],
      "metadata": {
        "id": "EFiz3onY8-i8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get coef_m and r2\n",
        "coef_m = results['COEF_M']\n",
        "r2 = results['R2']\n",
        "#get list of n well predicted voxels - n=3 in this case\n",
        "n = 3\n",
        "well_predicted_voxs = get_n_well_predicted_voxs(r2, n)\n",
        "print(n,\"best predicted voxels:\", well_predicted_voxs)"
      ],
      "metadata": {
        "id": "pZNsVpipUfsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coef_m.shape"
      ],
      "metadata": {
        "id": "oHTzZG0lUCG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Inspect the Weights"
      ],
      "metadata": {
        "id": "3aPWo8XxLVso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#get weights of a single voxel via the voxel number\n",
        "def get_weights_of_vox(coef_m, vox_num):\n",
        "  max_vox_coef_m = np.abs(coef_m[vox_num, :])\n",
        "  return max_vox_coef_m"
      ],
      "metadata": {
        "id": "mp9Te3TjAiet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###visualizing weights of best predicted voxel###\n",
        "import matplotlib.pyplot as plt\n",
        "#max_vox_coef_m = coef_m[well_predicted_voxs[0]] #- no abs value\n",
        "max_vox_coef_m = get_weights_of_vox(coef_m, well_predicted_voxs[0]) #- w abs value\n",
        "plt.hist(max_vox_coef_m)\n",
        "#plt.title('Coef_m of best predicted voxel')\n",
        "plt.title('Abs Value of Coef_m of best predicted voxel')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Hi4WTDkVaMwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coef_m.shape"
      ],
      "metadata": {
        "id": "tIFJlcqFUrgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "abs_coef_m = np.abs(coef_m)\n",
        "x = np.mean(abs_coef_m, axis=0)\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.xlabel('weight values', size=15)\n",
        "plt.ylabel('frequency of features', size=15)\n",
        "plt.hist(x)\n",
        "plt.savefig('ex_weights.png', dpi=250)"
      ],
      "metadata": {
        "id": "MgDts7glUtNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#visualize stats\n",
        "plt.hist(max_vox_coef_m)\n",
        "plt.axvspan(.0001,.00044, color='red', alpha=0.5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5Tf5qqXoN2tI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#takes a certain percent (in decimal) of the best weights/features, \n",
        "#and returns index of features in list format\n",
        "def get_percentage_best_features(max_vox_coef_m, decimal_percentage):\n",
        "  #changes to decimal percentage\n",
        "  if decimal_percentage > 1:\n",
        "    decimal_percentage /= 100\n",
        "  best_features = []\n",
        "  #sorts coef_m\n",
        "  max_vox_coef_m_copy = max_vox_coef_m.copy()\n",
        "  max_vox_coef_m_sorted = np.sort(max_vox_coef_m_copy)\n",
        "  #takes percent and turns into index at that percent\n",
        "  percentile = int(max_vox_coef_m.size*decimal_percentage)\n",
        "  #takes weights from index to highest\n",
        "  highest_weights = max_vox_coef_m_sorted[-percentile:]\n",
        "  for weight in highest_weights:\n",
        "    #gives index of highest weights, appends to list\n",
        "    loc = np.argwhere(max_vox_coef_m == weight)\n",
        "    best_feature = loc[0][0]\n",
        "    best_features.append(best_feature)\n",
        "  return best_features"
      ],
      "metadata": {
        "id": "Z9L4kW0hIpzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "percent_of_highest_weights = .25"
      ],
      "metadata": {
        "id": "aUnfKIeLLn5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get max_vox_coef_m of 1st best predicted voxel\n",
        "print(\"vox_num1:\", well_predicted_voxs[0])\n",
        "print(\"percentage_of_highest_weights:\", percent_of_highest_weights*100,\"%\")\n",
        "max_vox_coef_m1 = get_weights_of_vox(coef_m, well_predicted_voxs[0])\n",
        "best_features1 = get_percentage_best_features(max_vox_coef_m1, percent_of_highest_weights)\n",
        "print(\"total num of features1:\", max_vox_coef_m1.size)\n",
        "print(\"total num of best_features1:\", len(best_features1))"
      ],
      "metadata": {
        "id": "OXFFfz3iTZae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Choose Another Well Predicted Voxel"
      ],
      "metadata": {
        "id": "Qx0jQRQAWok3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#get max_vox_coef_m of 2nd best predicted voxel\n",
        "print(\"vox_num2:\", well_predicted_voxs[1])\n",
        "print(\"percentage_of_highest_weights:\", percent_of_highest_weights*100,\"%\")\n",
        "max_vox_coef_m2 = get_weights_of_vox(coef_m, well_predicted_voxs[1])\n",
        "best_features2 = get_percentage_best_features(max_vox_coef_m2, percent_of_highest_weights)\n",
        "print(\"total num of features2:\", max_vox_coef_m2.size)\n",
        "print(\"total num of best_features2:\", len(best_features2))"
      ],
      "metadata": {
        "id": "PlU910uvJLtl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get max_vox_coef_m of 3rd best predicted voxel\n",
        "print(\"vox_num3:\", well_predicted_voxs[2])\n",
        "print(\"percentage_of_highest_weights:\", percent_of_highest_weights*100,\"%\")\n",
        "max_vox_coef_m3 = get_weights_of_vox(coef_m, well_predicted_voxs[2])\n",
        "best_features3 = get_percentage_best_features(max_vox_coef_m3, percent_of_highest_weights)\n",
        "print(\"total num of features3:\", max_vox_coef_m3.size)\n",
        "print(\"total num of best_features3:\", len(best_features3))"
      ],
      "metadata": {
        "id": "AdOowhi5B40Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the shared features"
      ],
      "metadata": {
        "id": "UJ_HBNn7q6Oe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#counts num of shared features and gives list of those features\n",
        "#between 2 well-predicted voxels, 1 layer\n",
        "def get_shared_features(features1, features2):\n",
        "  shared_features = []\n",
        "  len_features = len(features1)\n",
        "  for i in range(len_features):\n",
        "    if features1[i] in features2:\n",
        "      shared_feature = features1[i]\n",
        "      shared_features.append(shared_feature)\n",
        "  shared_features.sort()\n",
        "  #return len_features, count, fraction_shared, shared_features\n",
        "  return shared_features"
      ],
      "metadata": {
        "id": "dByMuAWp6qak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#b/t 1 and 2\n",
        "shared_list1 = get_shared_features(best_features1, best_features2)\n",
        "print(len(shared_list1))\n",
        "#b/t 1 and 3\n",
        "shared_list2 = get_shared_features(best_features1, best_features3)\n",
        "print(len(shared_list2))\n",
        "#b/t 2 and 3\n",
        "shared_list3 = get_shared_features(best_features2, best_features3)\n",
        "print(len(shared_list3))"
      ],
      "metadata": {
        "id": "1nr9i052A1Qb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#b/t 1,2, and 3\n",
        "shared_all = get_shared_features(shared_list1, shared_list3)\n",
        "len(shared_all)"
      ],
      "metadata": {
        "id": "_WzZ9iHkOGBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for correlation\n",
        "def get_weights_of_features(weights, features):\n",
        "  feature_weights = []\n",
        "  for i in features:\n",
        "    feature_weights.append(weights[i])\n",
        "  return feature_weights"
      ],
      "metadata": {
        "id": "wB2oj6ixxzc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#v stacks specific vox_nums and their features - can be used for all voxs too\n",
        "#just set np.arange from 0 to num_voxs\n",
        "def vstack_vox_and_features(coef_m, vox_nums, features):\n",
        "  count = 0\n",
        "  arr = []\n",
        "  for num in vox_nums:\n",
        "    weights = get_weights_of_vox(coef_m, num)\n",
        "    feature_weights = get_weights_of_features(weights, features)\n",
        "    if count == 0: \n",
        "      arr.append(feature_weights)\n",
        "      count += 1\n",
        "    else:\n",
        "      arr = np.vstack((arr, feature_weights))\n",
        "  return arr"
      ],
      "metadata": {
        "id": "IFhQenpHyAvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arr = vstack_vox_and_features(coef_m, well_predicted_voxs, shared_all)"
      ],
      "metadata": {
        "id": "zdHeUYTiFOBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "arr_corrcoef = np.corrcoef(arr)\n",
        "print(\"arr.shape:\", arr.shape)\n",
        "print(\"arr_corrcoef.shape:\", arr_corrcoef.shape)\n",
        "d = sns.heatmap(arr_corrcoef, annot=True)\n",
        "d.set_title('3 best predicted voxels & their shared features correlation')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "F4mVMPz5GPvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1) get shared across ALL voxels (use 25% of highest weights) - now most common among all for 25%\n",
        "#2) get correlation\n",
        "#3) then set coef_m at those shared features to 0\n",
        "#4) then run veRSA - see how R2 is affected"
      ],
      "metadata": {
        "id": "U-0_TMe6Qbq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#--- OLD STUFF ---\n",
        "###1) shared across all \n",
        "#num_voxels = results['n_voxels']\n",
        "#percentage = .50\n",
        "#print(\"percentage of highest weighted features:\", percentage)\n",
        "#vox_nums = np.arange(num_voxels)\n",
        "#all_shared_features = []\n",
        "#for vox_num in vox_nums:\n",
        " # vox_coef_m = get_weights_of_vox(coef_m, vox_num)\n",
        "  #best_features = get_percentage_best_features(vox_coef_m, percentage)\n",
        "  #if vox_num == 0:\n",
        "   # all_shared_features = best_features\n",
        "  #else:\n",
        "    #all_shared_features = get_shared_features(all_shared_features, best_features)\n",
        "  #if vox_num % 10 == 0:\n",
        "   # print(\"all_shared_features len at vox num\", vox_num, \":\", len(all_shared_features))\n",
        "#print(\"total num voxels:\", len(vox_nums))\n",
        "###finding: even at 50% highest featuers - no feature is consistent across all vox!#\n",
        "#-------"
      ],
      "metadata": {
        "id": "0QMCAdDAccDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1) most consistent features at 25% level\n",
        "#gets counts for most consistent features - each count is at index of feature\n",
        "def get_feature_counts_for_most_consistent_across_voxels(coef_m, num_voxels, num_features, percentage):\n",
        "  #0 vector the length of num features\n",
        "  feature_counts = np.zeros(num_features)\n",
        "  #list of every vox num\n",
        "  vox_nums = np.arange(num_voxels)\n",
        "  #for each voxel\n",
        "  for vox_num in vox_nums:\n",
        "    #get weights of vox_num voxel\n",
        "    vox_coef_m = get_weights_of_vox(coef_m, vox_num)\n",
        "    #get percentage (25%) highest weighted features - list format\n",
        "    best_features = get_percentage_best_features(vox_coef_m, percentage)\n",
        "    for best_feature in best_features:\n",
        "      #adds 1 to that index every time that the feature is among top 25% for a given voxel\n",
        "      feature_counts[best_feature] += 1\n",
        "  #returns array of counts\n",
        "  return feature_counts\n",
        "\n",
        "def get_most_common_features_across_voxels(feature_counts, percentage):\n",
        "  #1D vector of size of features - use for future masking\n",
        "  feature_1_hot = np.zeros(len(feature_counts))\n",
        "  #copy and sort\n",
        "  feature_counts_copy = feature_counts.copy()\n",
        "  feature_counts_sorted = np.sort(feature_counts_copy)\n",
        "  #takes percent and turns into index at that percent\n",
        "  percentile = int(len(feature_counts)*percentage)\n",
        "  #takes weights from percentile to highest - most common percentage (15%) weighted features\n",
        "  best_feature_counts = feature_counts_sorted[-percentile:]\n",
        "  for num_counts in best_feature_counts:\n",
        "    #locations for which features have that num counts\n",
        "    locs = np.argwhere(feature_counts == num_counts)\n",
        "    #for every location with that number of counts - some may have same num counts\n",
        "    for loc in locs:\n",
        "      #adds a 1 to features with highest counts\n",
        "      feature_1_hot[loc] += 1\n",
        "  #for features w multiple counts - brings them down to 1 for mask\n",
        "  for i in range(len(feature_1_hot)):\n",
        "    if feature_1_hot[i] >1:\n",
        "      feature_1_hot[i] = 1\n",
        "  feature_idxs = []\n",
        "  #for features that have a 1 - get the index of those features\n",
        "  for i in range(len(feature_1_hot)):\n",
        "    if feature_1_hot[i] == 1:\n",
        "      feature_idxs.append(i)\n",
        "  return feature_1_hot, feature_idxs\n",
        "  "
      ],
      "metadata": {
        "id": "vT_maWRa02Rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#getting feature counts\n",
        "num_voxels = results['n_voxels']\n",
        "num_features = results['n_features']\n",
        "percentage = .25\n",
        "feature_counts = get_feature_counts_for_most_consistent_across_voxels(coef_m, num_voxels,num_features, percentage)"
      ],
      "metadata": {
        "id": "Td90B3Wm5NjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#visualize feature counts\n",
        "plt.hist(feature_counts)\n",
        "plt.axvspan(800, 1300, color='red', alpha=.2)\n",
        "plt.title('Feature Counts')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KvPCZG3alUiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#getting feature_1_hot and getting idxs of those features\n",
        "percentage = .15\n",
        "feature_1_hot, feature_idxs = get_most_common_features_across_voxels(feature_counts, percentage)"
      ],
      "metadata": {
        "id": "5fCnBhrE62BA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2) get correlation"
      ],
      "metadata": {
        "id": "zoV4kQyR_AMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "#num vox by num features\n",
        "arr = vstack_vox_and_features(coef_m, np.arange(0, 10), feature_idxs) #from 0 to num_voxels if want all\n",
        "#num sub by num sub\n",
        "arr_corr = np.corrcoef(arr)\n",
        "\n",
        "#visualize RSM\n",
        "d = sns.heatmap(arr_corr, annot=True)\n",
        "d.set_title(\"10 voxels &their shared features correlation\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZraWcAbVzGT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3) get coef_m masking in LOOCV"
      ],
      "metadata": {
        "id": "b_CE91_4_faD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_coef_m_mask(num_voxels, num_features, feature_1_hot):\n",
        "  #matrix of ones - size of coef_m\n",
        "  coef_m_mask = np.ones((num_voxels, num_features))\n",
        "  for i in range(feature_1_hot.size):\n",
        "    #if there is a 1 at featuer idx - that feature gets zeroed out in coef_m_mask\n",
        "    if feature_1_hot[i] == 1:\n",
        "      coef_m_mask[:,i] = 0\n",
        "  return coef_m_mask"
      ],
      "metadata": {
        "id": "-XWYRvjJ_lFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coef_m_mask = get_coef_m_mask(num_voxels, num_features, feature_1_hot)\n",
        "coef_m_mask.shape"
      ],
      "metadata": {
        "id": "J1dmbZxBBeXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4) see how veRSA is affected!"
      ],
      "metadata": {
        "id": "MdBhR0vIHQ6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#modified\n",
        "default_alphas = np.concatenate([np.array([1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1.0]), np.logspace(1, 5, 50)])\n",
        "\n",
        "def modified_leave_one_out_ridge(X, y, coef_m_mask, alphas=default_alphas, fit_intercept=True, normalize=True, mb=None):\n",
        "    '''\n",
        "        Construct predicted brain patterns by training on N-1 items, \n",
        "        and then predicting the held out item.\n",
        "        \n",
        "        X: model responses [numItems x numFeatures]\n",
        "        y: brain responses [numItems x numVoxels]\n",
        "    '''\n",
        "    n_items, n_features = X.shape\n",
        "    n_voxels = y.shape[1]\n",
        "    y_pred = np.zeros(y.shape)\n",
        "\n",
        "    ALPHAS = []\n",
        "    COEF_M = np.zeros((n_voxels, n_features))\n",
        "    INTERCEPT = []\n",
        "    for iter_count, test_idx in enumerate(progress_bar(range(n_items), total=n_items)):\n",
        "        train_idxs = np.ones(n_items) == True\n",
        "        train_idxs[test_idx] = False\n",
        "        test_idxs = ~train_idxs  \n",
        "\n",
        "        clf = RidgeCV(alphas=alphas, fit_intercept=fit_intercept)\n",
        "\n",
        "        if normalize:\n",
        "            scaler = StandardScaler()\n",
        "            scaler.fit(X[train_idxs])\n",
        "            X_train = scaler.transform(X[train_idxs])\n",
        "            X_test = scaler.transform(X[test_idxs])        \n",
        "        else:\n",
        "            X_train = X[train_idxs]\n",
        "            X_test = X[test_idxs]\n",
        "              \n",
        "        clf.fit(X_train, y[train_idxs])\n",
        "        ###change here start\n",
        "        clf.coef_ *= coef_m_mask\n",
        "        ###stop\n",
        "        y_pred[test_idxs] = clf.predict(X_test)\n",
        "        \n",
        "        ALPHAS.append(clf.alpha_)\n",
        "        COEF_M += clf.coef_\n",
        "        INTERCEPT.append(clf.intercept_)\n",
        "\n",
        "    ALPHAS = np.stack(ALPHAS)\n",
        "    COEF_M /= iter_count\n",
        "    INTERCEPT = np.stack(INTERCEPT)\n",
        "    R2 = r2_score(y, y_pred, multioutput='raw_values')\n",
        "    \n",
        "    return {\n",
        "        \"n_items\": n_items,\n",
        "        \"n_features\": n_features,\n",
        "        \"n_voxels\": y.shape[1],\n",
        "        \"ALPHAS\": ALPHAS,\n",
        "        \"COEF_M\": COEF_M,\n",
        "        \"INTERCEPT\": INTERCEPT,\n",
        "        \"R2\": R2,\n",
        "        \"y_pred\": y_pred\n",
        "        #\"diff_bt_coef_ms\": diff_bt_coef_ms\n",
        "    }"
      ],
      "metadata": {
        "id": "NMdoeb_oh8nr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#modified\n",
        "def modified_fit_encoding_model(betas, layer_name, coef_m_mask, model_name='alexnet',\n",
        "                       dataset='InanimateObjects', mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
        "          \n",
        "    #print(\"==> prepare images\")\n",
        "    imgs = prepare_images(dataset=dataset, mean=mean, std=std)\n",
        "    \n",
        "    #print(\"==> load pretrained model\")\n",
        "    model = models.__dict__[model_name](pretrained=True)\n",
        "\n",
        "    #print(\"==> extract activation map for the given layer\")\n",
        "    pred_rdms = {}\n",
        "    feat_rdms = {}\n",
        "    model.eval()   # <-- very important, freeze normalization stats, no dropout etc.\n",
        "    with FeatureExtractor(model, [layer_name]) as extractor:\n",
        "        features = extractor(imgs)\n",
        "        for layer_name,feat in features.items():\n",
        "            # retain spatial information, but flatten rows into a 1D feature vector\n",
        "            X = torch.flatten(feat, 1)\n",
        "            feat_rdm = 1 - np.corrcoef(X)\n",
        "            feat_rdms[layer_name] = feat_rdm\n",
        "            \n",
        "            #print(f\"==> fitting ridge regression model ({layer_name}) (numFeatures={X.shape[1]})\")\n",
        "            #this is the only part changed, modified LOOCV\n",
        "            results = modified_leave_one_out_ridge(X, betas, coef_m_mask, fit_intercept=True, normalize=False)\n",
        "            \n",
        "            # compute the predicted neural RDM\n",
        "            pred_rdm = 1 - np.corrcoef(results['y_pred'])\n",
        "            pred_rdms[layer_name] = pred_rdm\n",
        "              \n",
        "            # now do something with the rdms, e.g., save them for our split-half analysis\n",
        "          \n",
        "    return pred_rdms, feat_rdms, results"
      ],
      "metadata": {
        "id": "URDzVDDSldzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Highest"
      ],
      "metadata": {
        "id": "gFUFiM5lUzI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#getting feature counts for 1 sub\n",
        "pred_rdms, feat_rdms, results = fit_encoding_model(sub_betas, \n",
        "                                                   model_name='alexnet', \n",
        "                                                   layer_name='classifier.5',\n",
        "                                                   dataset='InanimateObjects', \n",
        "                                                   mean=[0.485, 0.456, 0.406], \n",
        "                                                   std=[0.229, 0.224, 0.225])\n",
        "\n",
        "num_voxels = results['n_voxels']\n",
        "num_features = results['n_features']\n",
        "coef_m = results['COEF_M']\n",
        "percentage = .25\n",
        "feature_counts = get_feature_counts_for_most_consistent_across_voxels(coef_m, num_voxels,num_features, percentage)\n",
        "\n"
      ],
      "metadata": {
        "id": "5_DJ4SKZHS24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "percentage = .10\n",
        "feature_1_hot, feature_idxs = get_most_common_features_across_voxels(feature_counts, percentage)"
      ],
      "metadata": {
        "id": "P6Off-c12HUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class5_feature_idxs = {}\n",
        "percentages = np.arange(.01 , 1, .01)\n",
        "for p in percentages:\n",
        "  feature_1_hot, feature_idxs = get_most_common_features_across_voxels(feature_counts, p)\n",
        "  class5_feature_idxs[round(p,2)] = feature_idxs"
      ],
      "metadata": {
        "id": "yCTw8ggg3Ch9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class5_feature_idxs.keys()"
      ],
      "metadata": {
        "id": "mYCh3iYv4FOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#getting feature counts for all subs - HERE HERE\n",
        "subs = [0,1,2,3,4,5,6,7,8,9]\n",
        "percentage_common = .25\n",
        "\n",
        "rdms, betas, reliability, image_names = load_brain_data('InanimateObjects')\n",
        "brain_region = 'EarlyV'\n",
        "layer_name = 'classifier.5'\n",
        "\n",
        "for sub in subs:\n",
        "\n",
        "  sub_betas = betas[brain_region][sub].transpose()\n",
        "  pred_rdms, feat_rdms, results = fit_encoding_model(sub_betas, \n",
        "                                                   model_name='alexnet', \n",
        "                                                   layer_name='classifier.5',\n",
        "                                                   dataset='InanimateObjects', \n",
        "                                                   mean=[0.485, 0.456, 0.406], \n",
        "                                                   std=[0.229, 0.224, 0.225])\n",
        "  num_voxels = results['n_voxels']\n",
        "  num_features = results['n_features']\n",
        "  coef_m = results['COEF_M']\n",
        "  feature_counts = get_feature_counts_for_most_consistent_across_voxels(coef_m, num_voxels,num_features, percentage_common)\n",
        "  if sub == 0:\n",
        "    all_feature_counts = np.zeros(num_features)\n",
        "    all_feature_counts = all_feature_counts + feature_counts\n",
        "  else:\n",
        "    all_feature_counts = all_feature_counts + feature_counts\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wH6OCzUcPpko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class5_feature_idxs2 = {}\n",
        "percentages = np.arange(.01 , 1, .01)\n",
        "for percentage in percentages:\n",
        "  feature_1_hot, feature_idxs = get_most_common_features_across_voxels(all_feature_counts, percentage)\n",
        "  class5_feature_idxs2[round(percentage,2)] = feature_idxs"
      ],
      "metadata": {
        "id": "zB--WbJIKink"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "def dict_save(dict, file_name):\n",
        "    with open(file_name + '.pickle', 'wb') as f:\n",
        "        pickle.dump(dict, f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def dict_load(dict_name):\n",
        "    with open(dict_name , 'rb') as f:\n",
        "              #+ '.pickle', 'rb') as f:\n",
        "        return pickle.load(f)"
      ],
      "metadata": {
        "id": "sXbMJqB0BxLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_save(class5_feature_idxs2, 'class5_feature_idxs2')"
      ],
      "metadata": {
        "id": "VjduTczRBx5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_counts_x = np.arange(0, len(feature_counts), 1)\n",
        "plt.hist(feature_counts)"
      ],
      "metadata": {
        "id": "_eKcu3XbkvZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = [i for i in feature_counts if i > 1500]\n",
        "len(x)"
      ],
      "metadata": {
        "id": "2Fdk43nTmIi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lesion highest - r2 scores\n",
        "r2scores = {}\n",
        "percentages = np.arange(0.05,.3,.05)\n",
        "for percentage in percentages:\n",
        "  print('PERCENTAGE:', percentage)\n",
        "  feature_1_hot, feature_idxs = get_most_common_features_across_voxels(feature_counts, percentage)\n",
        "  coef_m_mask = get_coef_m_mask(num_voxels, num_features, feature_1_hot)\n",
        "  pred_rdms, feat_rdms, results = modified_fit_encoding_model(sub_betas, \n",
        "                                                   model_name='alexnet', \n",
        "                                                   layer_name='classifier.5',\n",
        "                                                   coef_m_mask = coef_m_mask,\n",
        "                                                   dataset='InanimateObjects', \n",
        "                                                   mean=[0.485, 0.456, 0.406], \n",
        "                                                   std=[0.229, 0.224, 0.225])\n",
        "  r2scores[str(round(percentage,2))] = results['R2']\n",
        "  print(\"r2 score mean:\", np.mean(results['R2']),\"\\n\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "I0WL8OrBHcEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FINAL ANALYSIS"
      ],
      "metadata": {
        "id": "fn_0TqFC8GFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#lesioning highest - r2 scores - results - FINAL - #EarlyV, InanimateObjects\n",
        "percentage_common = .25\n",
        "percentages = np.arange(.01, 1, .01)\n",
        "subs = [0,1,2,3,4,5,6,7,8,9]\n",
        "subs = [0,1]\n",
        "rdms, betas, reliability, image_names = load_brain_data('InanimateObjects')\n",
        "brain_region = 'EarlyV'\n",
        "layer_name = 'classifier.5'\n",
        "\n",
        "no_lesion_r2s = {}\n",
        "lesioned_r2s = {}\n",
        "percentage_lesioned = {}\n",
        "\n",
        "for sub in subs:\n",
        "  sub_betas = betas[brain_region][sub].transpose()\n",
        "  pred_rdms, feat_rdms, results = fit_encoding_model(sub_betas, \n",
        "                                                   model_name='alexnet', \n",
        "                                                   layer_name=layer_name,\n",
        "                                                   dataset='InanimateObjects', \n",
        "                                                   mean=[0.485, 0.456, 0.406], \n",
        "                                                   std=[0.229, 0.224, 0.225])\n",
        "  coef_m = results['COEF_M']\n",
        "  num_voxels = results['n_voxels']\n",
        "  num_features = results['n_features']\n",
        "  original_r2 = results['R2'].mean()\n",
        "  no_lesion_r2s[\"sub\"+str(sub)+\"_\"+layer_name] = original_r2\n",
        "  print(\"SUB:\",str(sub), \"original_r2:\", original_r2)\n",
        "  half_original_r2 = original_r2 / 2\n",
        "\n",
        "  #feature counrs vector\n",
        "  feature_counts = get_feature_counts_for_most_consistent_across_voxels(coef_m, num_voxels, num_features, percentage_common)\n",
        "\n",
        "  #1 percent to 100 percent - by increments of 1 percent\n",
        "  for percentage in percentages:\n",
        "    feature_1_hot, feature_idxs = get_most_common_features_across_voxels(feature_counts, percentage)\n",
        "    coef_m_mask = get_coef_m_mask(num_voxels, num_features, feature_1_hot)\n",
        "    #print(\"sub\"+str(sub)+\", percentage\"+ str(percentage))\n",
        "    pred_rdms, feat_rdms, results = modified_fit_encoding_model(sub_betas, \n",
        "                                                   model_name='alexnet', \n",
        "                                                   layer_name=layer_name,\n",
        "                                                   coef_m_mask = coef_m_mask,\n",
        "                                                   dataset='InanimateObjects', \n",
        "                                                   mean=[0.485, 0.456, 0.406], \n",
        "                                                   std=[0.229, 0.224, 0.225])\n",
        "\n",
        "    if results['R2'].mean() <= 0:\n",
        "      lesioned_r2s[\"sub\"+str(sub)+\"_\"+layer_name] = results['R2'].mean()\n",
        "      percentage_lesioned[\"sub\"+str(sub)+\"_\"+layer_name] = percentage\n",
        "      print(\"Sub\", str(sub), \"Final Percentage:\", str(percentage))\n",
        "      print(\"Sub\", str(sub), \"Final R2:\", str(results['R2'].mean()))\n",
        "      break\n"
      ],
      "metadata": {
        "id": "1E4uFgHL7oIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "no_lesion_r2s.keys()"
      ],
      "metadata": {
        "id": "lpIdByvylwhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "END FINAL ANALYSIS"
      ],
      "metadata": {
        "id": "68Ye12GW-iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r2scores.keys()"
      ],
      "metadata": {
        "id": "Mva3a_JAS0hF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#visualize r2 and percent lesioned\n",
        "percentages = [0, 0.05, 0.1 , 0.15, 0.2 , 0.25]\n",
        "#list with mean of original r2 already there! - append to it!\n",
        "r2scores_means = [np.mean(r2)]\n",
        "for p in pcents:\n",
        "  r2scores_mean = np.mean(r2scores[str(p)])\n",
        "  r2scores_means.append(r2scores_mean)\n",
        "\n",
        "plt.scatter(percentages, r2scores_means)\n",
        "plt.xlabel(\"percentage of features lesioned\")\n",
        "plt.ylabel(\"r2 score\")\n",
        "plt.title('highest')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BM9ysoeUTa63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lesion highest - group half RSA\n",
        "percentages = np.arange(0.05,.3,.05)\n",
        "brain_region = 'EarlyV'\n",
        "layer_name = \"classifier.5\"\n",
        "neural_rdms = rdms[brain_region]\n",
        "subs = [0,1,2,3,4,5,6,7,8,9]\n",
        "group_half_rsa_means = np.zeros(percentages.shape)\n",
        "\n",
        "#i becasuse idx i is ued later!!\n",
        "for i in range(len(percentages)):\n",
        "  percentage = percentages[i]\n",
        "  for sub in subs:\n",
        "    print(\"\\n\\nPercentage:\", percentage, \", Sub:\", sub)\n",
        "    sub_betas = betas['EarlyV'][sub].transpose()\n",
        "    num_voxels = sub_betas.shape[1]\n",
        "    #num_features = ??? -- have to update this when do for other layers!!!\n",
        "\n",
        "    #modified LOOCV with lesioning\n",
        "    feature_1_hot, feature_idxs = get_most_common_features_across_voxels(feature_counts, percentage)\n",
        "    coef_m_mask = get_coef_m_mask(num_voxels, num_features, feature_1_hot)\n",
        "    pred_rdms, feat_rdms, results = modified_fit_encoding_model(sub_betas, \n",
        "                                                   model_name='alexnet', \n",
        "                                                   layer_name='classifier.5',\n",
        "                                                   coef_m_mask = coef_m_mask,\n",
        "                                                   dataset='InanimateObjects', \n",
        "                                                   mean=[0.485, 0.456, 0.406], \n",
        "                                                   std=[0.229, 0.224, 0.225])\n",
        "    \n",
        "    #making pred_rdms with 10x72x72\n",
        "    pred_rdm = pred_rdms[layer_name].reshape((1,72,72))\n",
        "    feat_rdm = feat_rdms[layer_name].reshape((1,72,72))\n",
        "    if sub == 0:\n",
        "      pred_rdms2 = pred_rdm   #pred_rdms1 so that pred_rdms doesnt get relabeled!\n",
        "      feat_rdms2 = feat_rdm   #feat_rdm1 so that feat_rdms doesnt get relabeled!\n",
        "    else:\n",
        "      pred_rdms2 = np.concatenate((pred_rdms2, pred_rdm), axis=0)\n",
        "      feat_rdms2 = np.concatenate((feat_rdms2, feat_rdm), axis=0)\n",
        "\n",
        "  print(\"pred_rdms2.shape\", pred_rdms2.shape)\n",
        "\n",
        "  assert pred_rdms2.shape == neural_rdms.shape, \"oops, expected same shapes\"\n",
        "\n",
        "  #getting split halfs\n",
        "  N = pred_rdms2.shape[0]\n",
        "  splits = get_split_halves(N)\n",
        "  print(f\"N={N}, num_splits={len(splits)}\")\n",
        "\n",
        "  #getting df\n",
        "  model_name = \"alexnet\"\n",
        "  df = pd.DataFrame(columns=['analysis','model_name','dataset','brain_region',\n",
        "                           'layer_name', 'split_num', 'split_idxs', 'group', 'pearsonr'])\n",
        "\n",
        "  for split_num, (group1, group2) in enumerate(progress_bar(splits)):\n",
        "\n",
        "    idx_group1 = np.array(group1)\n",
        "    idx_group2 = np.array(group2)                                \n",
        "\n",
        "    brain_rdm1 = compute_avg_rdm(neural_rdms[idx_group1]).squeeze()\n",
        "    brain_rdm2 = compute_avg_rdm(neural_rdms[idx_group2]).squeeze()\n",
        "    \n",
        "    pred_rdm1 = compute_avg_rdm(pred_rdms2[idx_group1]).squeeze()\n",
        "    pred_rdm2 = compute_avg_rdm(pred_rdms2[idx_group2]).squeeze()                                \n",
        "\n",
        "    sim1 = compare_rdms(brain_rdm1, pred_rdm1)\n",
        "    sim2 = compare_rdms(brain_rdm2, pred_rdm2)\n",
        "\n",
        "    # record the results\n",
        "    df = update_df(df, brain_region, model_name, layer_name, split_num, \n",
        "                   idx_group1, 'group1', sim1)\n",
        "\n",
        "    df = update_df(df, brain_region, model_name, layer_name, split_num, \n",
        "                   idx_group2, 'group2', sim2)\n",
        "    \n",
        "  mean, lower, upper = compute_fisherz_ci(df.pearsonr)\n",
        "  print(f\"mean={mean:3.3f}, 95% CI=[{lower:3.3f},{upper:3.3f}]\")\n",
        "  group_half_rsa_means[i] = (mean)\n",
        "\n"
      ],
      "metadata": {
        "id": "Ja3B9Pxxo534"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#visualize group half means and percent lesioned\n",
        "group_half_means = [0.455, 0.23450298,  0.15100923,  0.06443817,  0.01683275, -0.02467095]\n",
        "percentages = [0, 0.05, 0.1 , 0.15, 0.2 , 0.25]\n",
        "\n",
        "plt.scatter(percentages, group_half_means)\n",
        "plt.title('highest')\n",
        "plt.yticks(np.arange(0,.6,.1))\n",
        "plt.xlabel('percetage lesioned')\n",
        "plt.ylabel('group half rsa mean')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1TgEqkcUO2kE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#maybe most common among all voxels is not a good metric - maybe do highest within voxels for each voxel"
      ],
      "metadata": {
        "id": "Gq9R-pLHUdWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random"
      ],
      "metadata": {
        "id": "nl047EwySRg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#what happens when lesion a random percentage??"
      ],
      "metadata": {
        "id": "nctXqdHDQ22p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "coef_m_mask = np.ones((num_voxels, num_features))\n",
        "features = list(np.arange(0,4096))\n",
        "integer = int(len(features)*.15)\n",
        "random_features = random.sample(features, integer)\n",
        "for i in range(len(features)):\n",
        "  if features[i] in random_features:\n",
        "    coef_m_mask[:,i] = 0\n"
      ],
      "metadata": {
        "id": "l4VHxQSkoZrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "def get_random_features_coef_m_mask(num_voxels, num_features, percentage):\n",
        "  #mask size of coef_m - all ones\n",
        "  coef_m_mask = np.ones((num_voxels, num_features))\n",
        "  #0,1,2...,4094,4095\n",
        "  features = list(np.arange(0,num_features))\n",
        "  #random sample of percentile many features in features\n",
        "  percentile = int(len(features)*.15)\n",
        "  random_features = random.sample(features, percentile)\n",
        "  #if a feature is in the random features sample, then coef_m mask gets 0 at that col\n",
        "  for i in range(len(features)):\n",
        "    if features[i] in random_features:\n",
        "      coef_m_mask[:,i] = 0\n",
        "  return coef_m_mask\n",
        "\n"
      ],
      "metadata": {
        "id": "---Q7fapTMy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lesion random - r2 scores\n",
        "randr2scores = {}\n",
        "features = list(np.arange(0,4096))\n",
        "percentages = np.arange(0.05,.3,.05)\n",
        "for percentage in percentages:\n",
        "  print('PERCENTAGE:', percentage)\n",
        "  coef_m_mask = get_random_features_coef_m_mask(num_voxels, num_features, percentage)\n",
        "  #integer = int(len(features)*percentage)\n",
        "  #random_features = random.sample(features, integer)\n",
        "  #coef_m_mask = np.ones((num_voxels, num_features))\n",
        "  #for i in range(len(features)):\n",
        "    #if features[i] in random_features:\n",
        "      #coef_m_mask[:,i] = 0\n",
        "  pred_rdms, feat_rdms, results = modified_fit_encoding_model(sub_betas, \n",
        "                                                   model_name='alexnet', \n",
        "                                                   layer_name='classifier.5',\n",
        "                                                   coef_m_mask = coef_m_mask,\n",
        "                                                   dataset='InanimateObjects', \n",
        "                                                   mean=[0.485, 0.456, 0.406], \n",
        "                                                   std=[0.229, 0.224, 0.225])\n",
        "  randr2scores[str(round(percentage,2))] = results['R2']\n",
        "  print(\"r2 score mean:\", np.mean(results['R2']),\"\\n\\n\")"
      ],
      "metadata": {
        "id": "U4pQBvM0uMDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "randr2scores.keys()"
      ],
      "metadata": {
        "id": "FczM0vZNxpK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#visualize r2 and percent lesioned\n",
        "percentages = np.arange(0.05,.3,.05)\n",
        "randr2scores_means = [np.mean(r2)]\n",
        "for p in percentages:\n",
        "  randr2scores_mean = np.mean(randr2scores[str(p)])\n",
        "  randr2scores_means.append(randr2scores_mean)"
      ],
      "metadata": {
        "id": "CaodzInYxrPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(pcents, randr2scores_means)\n",
        "plt.xlabel(\"percentage of features lesioned\")\n",
        "plt.ylabel(\"r2 score\")\n",
        "plt.title('random')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OzN22R9myJRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lesion random - group half RSA\n",
        "brain_region = 'EarlyV'\n",
        "layer_name = \"classifier.5\"\n",
        "neural_rdms = rdms[brain_region]\n",
        "subs = [0,1,2,3,4,5,6,7,8,9]\n",
        "percentages = np.arange(0.05,.3,.05)\n",
        "group_half_rsa_means = np.zeros(percentages.shape)\n",
        "\n",
        "#i becasuse idx i is ued later!!\n",
        "for i in range(len(percentages)):\n",
        "  percentage = percentages[i]\n",
        "  for sub in subs:\n",
        "    print(\"\\n\\nPercentage:\", percentage, \", Sub:\", sub)\n",
        "    sub_betas = betas['EarlyV'][sub].transpose()\n",
        "    num_voxels = sub_betas.shape[1]\n",
        "    #num_features = ??? -- have to update this when do for other layers!!!\n",
        "\n",
        "    #modified LOOCV with lesioning\n",
        "    coef_m_mask = get_random_features_coef_m_mask(num_voxels, num_features, percentage)\n",
        "    pred_rdms, feat_rdms, results = modified_fit_encoding_model(sub_betas, \n",
        "                                                   model_name='alexnet', \n",
        "                                                   layer_name='classifier.5',\n",
        "                                                   coef_m_mask = coef_m_mask,\n",
        "                                                   dataset='InanimateObjects', \n",
        "                                                   mean=[0.485, 0.456, 0.406], \n",
        "                                                   std=[0.229, 0.224, 0.225])\n",
        "    pred_rdm = pred_rdms[layer_name].reshape((1,72,72))\n",
        "    feat_rdm = feat_rdms[layer_name].reshape((1,72,72))\n",
        "    if sub == 0:\n",
        "      pred_rdms3 = pred_rdm   #pred_rdms1 so that pred_rdms doesnt get relabeled!\n",
        "      feat_rdms3 = feat_rdm   #feat_rdm1 so that feat_rdms doesnt get relabeled!\n",
        "    else:\n",
        "      pred_rdms3 = np.concatenate((pred_rdms3, pred_rdm), axis=0)\n",
        "      feat_rdms3 = np.concatenate((feat_rdms3, feat_rdm), axis=0)\n",
        "\n",
        "  print(\"pred_rdms3.shape\", pred_rdms3.shape)\n",
        "\n",
        "  assert pred_rdms3.shape == neural_rdms.shape, \"oops, expected same shapes\"\n",
        "\n",
        "  #getting split halfs\n",
        "  N = pred_rdms3.shape[0]\n",
        "  splits = get_split_halves(N)\n",
        "  print(f\"N={N}, num_splits={len(splits)}\")\n",
        "\n",
        "  #getting df\n",
        "  model_name = \"alexnet\"\n",
        "  df = pd.DataFrame(columns=['analysis','model_name','dataset','brain_region',\n",
        "                           'layer_name', 'split_num', 'split_idxs', 'group', 'pearsonr'])\n",
        "\n",
        "  for split_num, (group1, group2) in enumerate(progress_bar(splits)):\n",
        "\n",
        "    idx_group1 = np.array(group1)\n",
        "    idx_group2 = np.array(group2)                                \n",
        "\n",
        "    brain_rdm1 = compute_avg_rdm(neural_rdms[idx_group1]).squeeze()\n",
        "    brain_rdm2 = compute_avg_rdm(neural_rdms[idx_group2]).squeeze()\n",
        "    \n",
        "    pred_rdm1 = compute_avg_rdm(pred_rdms3[idx_group1]).squeeze()\n",
        "    pred_rdm2 = compute_avg_rdm(pred_rdms3[idx_group2]).squeeze()                                \n",
        "\n",
        "    sim1 = compare_rdms(brain_rdm1, pred_rdm1)\n",
        "    sim2 = compare_rdms(brain_rdm2, pred_rdm2)\n",
        "\n",
        "    # record the results\n",
        "    df = update_df(df, brain_region, model_name, layer_name, split_num, \n",
        "                   idx_group1, 'group1', sim1)\n",
        "\n",
        "    df = update_df(df, brain_region, model_name, layer_name, split_num, \n",
        "                   idx_group2, 'group2', sim2)\n",
        "    \n",
        "  mean, lower, upper = compute_fisherz_ci(df.pearsonr)\n",
        "  print(f\"mean={mean:3.3f}, 95% CI=[{lower:3.3f},{upper:3.3f}]\")\n",
        "  group_half_rsa_means[i] = (mean)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pAXnWzEoMc-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "group_half_rsa_means"
      ],
      "metadata": {
        "id": "Bc-jXdTSSXx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#visualize group_half_means and percentages lesioned \n",
        "percentages = [0, 0.05, 0.1 , 0.15, 0.2 , 0.25]\n",
        "group_half_means = [.455, 0.41972714, 0.43015425, 0.42299897, 0.42655967, 0.43461551]\n",
        "plt.scatter(percentages, group_half_means)\n",
        "plt.title('random')\n",
        "plt.yticks(np.arange(0,.6,.1))\n",
        "plt.xlabel('percetage lesioned')\n",
        "plt.ylabel('group half rsa mean')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XFBryWN4SQbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lowest"
      ],
      "metadata": {
        "id": "hj_UMcFxUsr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_least_common_features(feature_counts, percentage):\n",
        "  #1D vector of size of features - use for future masking\n",
        "  feature_1_hot = np.zeros(len(feature_counts))\n",
        "  #copy and sort\n",
        "  feature_counts_copy = feature_counts.copy()\n",
        "  feature_counts_sorted = np.sort(feature_counts_copy)\n",
        "  #takes percent and turns into index at that percent\n",
        "  percentile = int(len(feature_counts)*percentage)\n",
        "  #takes weights from 0 to percentile - least common percentage(15%) weighted features\n",
        "  worst_feature_counts = feature_counts_sorted[:percentile]\n",
        "  #locations for which features have that num counts\n",
        "  for num_counts in worst_feature_counts:\n",
        "    locs = np.argwhere(feature_counts == num_counts)\n",
        "    #for every location with that number of counts - some may have same num counts\n",
        "    for loc in locs:\n",
        "      feature_1_hot[loc] += 1\n",
        "  for i in range(len(feature_1_hot)):\n",
        "    if feature_1_hot[i] >1:\n",
        "      feature_1_hot[i] = 1\n",
        "  feature_idxs = []\n",
        "  #for features that have a 1 - get the index of those features\n",
        "  for i in range(len(feature_1_hot)):\n",
        "    if feature_1_hot[i] == 1:\n",
        "      feature_idxs.append(i)\n",
        "  return feature_1_hot, feature_idxs\n",
        "\n"
      ],
      "metadata": {
        "id": "IhZAeL2oWVJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lesion lowest - veRSA r2 scores\n",
        "lowestr2scores = {}\n",
        "percentages = np.arange(0.05,.3,.05)\n",
        "for percentage in percentages:\n",
        "  least_feature_1_hot, least_feature_idxs = get_least_common_features(feature_counts, percentage)\n",
        "  coef_m_mask = get_coef_m_mask(num_voxels, num_features, least_feature_1_hot)\n",
        "  pred_rdms, feat_rdms, results = modified_fit_encoding_model(sub_betas, \n",
        "                                                   model_name='alexnet', \n",
        "                                                   layer_name='classifier.5',\n",
        "                                                   coef_m_mask = coef_m_mask,\n",
        "                                                   dataset='InanimateObjects', \n",
        "                                                   mean=[0.485, 0.456, 0.406], \n",
        "                                                   std=[0.229, 0.224, 0.225])\n",
        "  lowestr2scores[str(round(percentage,2))] = results['R2']\n",
        "  print(\"r2 score mean:\", np.mean(results['R2']),\"\\n\\n\")"
      ],
      "metadata": {
        "id": "-2054i0kXl42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lowestr2scores.keys()"
      ],
      "metadata": {
        "id": "NCUoWr9CkdO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "percentages = np.arange(0.05,.3,.05)\n",
        "#pcents = np.arange(.02,1.02,.02)\n",
        "lowestr2scores_means = [np.mean(r2)]\n",
        "for p in pcents:\n",
        "  lowestr2scores_mean = np.mean(lowestr2scores[str(p)])\n",
        "  lowestr2scores_means.append(lowestr2scores_mean)"
      ],
      "metadata": {
        "id": "b89nPfFRjUd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pcents = ['0.0', '0.01', '0.02', '0.03', '0.04', '0.05', '0.06', '0.07', '0.08', '0.09']\n",
        "pcents = [float(x) for x in pcents]\n",
        "pcents"
      ],
      "metadata": {
        "id": "aldy8znFlMbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(pcents, lowestr2scores_means)\n",
        "plt.xlabel(\"percentage of features lesioned\")\n",
        "plt.ylabel(\"r2 score\")\n",
        "plt.title('lowest')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MC-ZPTXyjhsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lesion lowest - group half RSA\n",
        "brain_region = 'EarlyV'\n",
        "layer_name = \"classifier.5\"\n",
        "neural_rdms = rdms[brain_region]\n",
        "subs = [0,1,2,3,4,5,6,7,8,9]\n",
        "percentages = np.arange(0.05,.3,.05)\n",
        "group_half_rsa_means = np.zeros(percentages.shape)\n",
        "\n",
        "#i becasuse idx i is ued later!!\n",
        "for i in range(len(percentages)):\n",
        "  percentage = percentages[i]\n",
        "  for sub in subs:\n",
        "    print(\"\\n\\nPercentage:\", percentage, \", Sub:\", sub)\n",
        "    sub_betas = betas['EarlyV'][sub].transpose()\n",
        "    num_voxels = sub_betas.shape[1]\n",
        "    #num_features = ??? -- have to update this when do for other layers!!!\n",
        "\n",
        "    #modified LOOCV with lesioning\n",
        "    least_feature_1_hot, least_feature_idxs = get_least_common_features(feature_counts, percentage)\n",
        "    coef_m_mask = get_coef_m_mask(num_voxels, num_features, least_feature_1_hot)\n",
        "    pred_rdms, feat_rdms, results = modified_fit_encoding_model(sub_betas, \n",
        "                                                   model_name='alexnet', \n",
        "                                                   layer_name='classifier.5',\n",
        "                                                   coef_m_mask = coef_m_mask,\n",
        "                                                   dataset='InanimateObjects', \n",
        "                                                   mean=[0.485, 0.456, 0.406], \n",
        "                                                   std=[0.229, 0.224, 0.225])\n",
        "    pred_rdm = pred_rdms[layer_name].reshape((1,72,72))\n",
        "    feat_rdm = feat_rdms[layer_name].reshape((1,72,72))\n",
        "    if sub == 0:\n",
        "      pred_rdms4 = pred_rdm   #pred_rdms1 so that pred_rdms doesnt get relabeled!\n",
        "      feat_rdms4 = feat_rdm   #feat_rdm1 so that feat_rdms doesnt get relabeled!\n",
        "    else:\n",
        "      pred_rdms4 = np.concatenate((pred_rdms4, pred_rdm), axis=0)\n",
        "      feat_rdms4 = np.concatenate((feat_rdms4, feat_rdm), axis=0)\n",
        "  print(\"pred_rdms4.shape\", pred_rdms4.shape)\n",
        "\n",
        "  assert pred_rdms4.shape == neural_rdms.shape, \"oops, expected same shapes\"\n",
        "\n",
        "  #getting split halfs\n",
        "  N = pred_rdms3.shape[0]\n",
        "  splits = get_split_halves(N)\n",
        "  print(f\"N={N}, num_splits={len(splits)}\")\n",
        "\n",
        "  #getting df\n",
        "  model_name = \"alexnet\"\n",
        "  df = pd.DataFrame(columns=['analysis','model_name','dataset','brain_region',\n",
        "                           'layer_name', 'split_num', 'split_idxs', 'group', 'pearsonr'])\n",
        "\n",
        "  for split_num, (group1, group2) in enumerate(progress_bar(splits)):\n",
        "\n",
        "    idx_group1 = np.array(group1)\n",
        "    idx_group2 = np.array(group2)                                \n",
        "\n",
        "    brain_rdm1 = compute_avg_rdm(neural_rdms[idx_group1]).squeeze()\n",
        "    brain_rdm2 = compute_avg_rdm(neural_rdms[idx_group2]).squeeze()\n",
        "    \n",
        "    pred_rdm1 = compute_avg_rdm(pred_rdms4[idx_group1]).squeeze()\n",
        "    pred_rdm2 = compute_avg_rdm(pred_rdms4[idx_group2]).squeeze()                                \n",
        "\n",
        "    sim1 = compare_rdms(brain_rdm1, pred_rdm1)\n",
        "    sim2 = compare_rdms(brain_rdm2, pred_rdm2)\n",
        "\n",
        "    # record the results\n",
        "    df = update_df(df, brain_region, model_name, layer_name, split_num, \n",
        "                   idx_group1, 'group1', sim1)\n",
        "\n",
        "    df = update_df(df, brain_region, model_name, layer_name, split_num, \n",
        "                   idx_group2, 'group2', sim2)\n",
        "    \n",
        "  mean, lower, upper = compute_fisherz_ci(df.pearsonr)\n",
        "  print(f\"mean={mean:3.3f}, 95% CI=[{lower:3.3f},{upper:3.3f}]\")\n",
        "  group_half_rsa_means[i] = (mean)"
      ],
      "metadata": {
        "id": "qHLjUA8jQGKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "group_half_rsa_means"
      ],
      "metadata": {
        "id": "m6PPIwwbSqqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#visualize group_half_means and percentages lesioned\n",
        "percentages = [0, 0.05, 0.1 , 0.15, 0.2 , 0.25]\n",
        "#original at index 0 - then lesioned ones\n",
        "group_half_means = [.455, 0.45519225, 0.45519225, 0.45519225, 0.45519225, 0.45519225]\n",
        "plt.scatter(percentages, group_half_means)\n",
        "plt.yticks(np.arange(0,.6,.1))\n",
        "plt.title('lowest')\n",
        "plt.xlabel('percetage lesioned')\n",
        "plt.ylabel('group half rsa mean')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Y6AeSqMCSp25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#now: use r2 scores - pseudo code:\n",
        "#for i in np.arange(.01, 1, .01) (percents used)\n",
        "#half_r2 = np.r2scores_mean / 2\n",
        "#if r2 <= half_r2, break\n",
        "#return percentage..."
      ],
      "metadata": {
        "id": "7Avma7mRi0q_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "----------------------------------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "5Q2-mf9s-VUQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1) features most consistent at 25% level\n",
        "num_voxels = results['n_voxels']\n",
        "num_features = results['n_features']\n",
        "percentage = .25\n",
        "#feature_counts = np.zeros((num_voxels, num_features))\n",
        "feature_counts = np.zeros(num_features)\n",
        "\n",
        "print(\"percentage of highest weighted features:\", percentage)\n",
        "vox_nums = np.arange(num_voxels)\n",
        "\n",
        "#goes thru each voxel\n",
        "for vox_num in vox_nums:\n",
        "  vox_coef_m = get_weights_of_vox(coef_m, vox_num)\n",
        "  #gets the percentage (25%) highest weighted features\n",
        "  best_features = get_percentage_best_features(vox_coef_m, percentage)\n",
        "  for best_feature in best_features:\n",
        "    #adds 1 each time that the feature is among top percentage (25%) for a given voxel\n",
        "    feature_counts[best_feature] += 1"
      ],
      "metadata": {
        "id": "49kRm9D-Pfg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#15% most common features across all voxels\n",
        "percentage = .15\n",
        "#1D vector for future coef_m mask\n",
        "feature_1_hot = np.zeros(len(feature_counts))\n",
        "#copy and sort\n",
        "feature_counts_copy = feature_counts.copy()\n",
        "feature_counts_sorted = np.sort(feature_counts_copy)\n",
        "#takes percent and turns into index at that percent\n",
        "percentile = int(len(feature_counts)*percentage)\n",
        "#takes weights from percentile to highest - most common 15% weighted features\n",
        "best_feature_counts = feature_counts_sorted[-percentile:]\n",
        "#the number of counts at each feature\n",
        "for num_counts in best_feature_counts:\n",
        "  #locations for which features have that num counts\n",
        "  locs = np.argwhere(feature_counts == num_counts)\n",
        "  for loc in locs:\n",
        "    #adds a 1 to features with highest counts\n",
        "    feature_1_hot[loc] += 1\n",
        "\n"
      ],
      "metadata": {
        "id": "bKLCSYssogcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for features w multiple counts - brings them down to 1 for mask\n",
        "for i in range(len(feature_1_hot)):\n",
        "  if feature_1_hot[i] >1:\n",
        "    feature_1_hot[i] = 1"
      ],
      "metadata": {
        "id": "S9t-0PMQ7oAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"feature_1_hot.shape:\",  feature_1_hot.shape)\n",
        "print(\"feature_1_hot.sum()\", feature_1_hot.sum())\n",
        "print(4096*.15)"
      ],
      "metadata": {
        "id": "xqjRFM_r7lws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3) coef__m masking"
      ],
      "metadata": {
        "id": "qzw5qqzY_Fu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# matrix size of coef_m\n",
        "coef_m_1_hot = np.zeros((coef_m.shape))\n",
        "coef_m_1_hot.shape"
      ],
      "metadata": {
        "id": "u0gPB7Q-1UU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#if it is a significant feature - the coef_m 0's it out!\n",
        "for i in range(feature_1_hot.size):\n",
        "  if feature_1_hot[i] == 1:\n",
        "    coef_m_1_hot[:, i] = 0"
      ],
      "metadata": {
        "id": "rw9NHs_Y07f7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#use mat mult!!! - 1 hot encoding\n",
        "#for sig features - have columns in coef_m of that feature = 1\n",
        "#for non sig features - have cols in coef_m of that feature = 0\n",
        "#then mat mult - gives what i want!!!!"
      ],
      "metadata": {
        "id": "nUnSLOJ-hX3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4) see how veRSA is affected!"
      ],
      "metadata": {
        "id": "jd9vRjlkpyN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "----------------------------------------------------------------"
      ],
      "metadata": {
        "id": "HaBCsHrLYQMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred_rdms, feat_rdms, results1 = fit_encoding_model(sub_betas, \n",
        "                                                   model_name='alexnet', \n",
        "                                                   layer_name='classifier.5',\n",
        "                                                   dataset='InanimateObjects', \n",
        "                                                   mean=[0.485, 0.456, 0.406], \n",
        "                                                   std=[0.229, 0.224, 0.225])"
      ],
      "metadata": {
        "id": "WLsjS5NYl-_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_rdms, feat_rdms, results2 = modified_fit_encoding_model(sub_betas, \n",
        "                                                   model_name='alexnet', \n",
        "                                                   layer_name='classifier.5',\n",
        "                                                   dataset='InanimateObjects', \n",
        "                                                   mean=[0.485, 0.456, 0.406], \n",
        "                                                   std=[0.229, 0.224, 0.225])"
      ],
      "metadata": {
        "id": "PEt8PjGXmBPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results2.keys()"
      ],
      "metadata": {
        "id": "3ztGDSqMmPkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r2.mean()"
      ],
      "metadata": {
        "id": "Laas4pVU1cr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this is same as original mean\n",
        "r21 = results1['R2']\n",
        "r21.mean()"
      ],
      "metadata": {
        "id": "QgfN2WPV_l4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this is modified (mask out original)\n",
        "r22 = results2['R2']\n",
        "r22.mean()"
      ],
      "metadata": {
        "id": "W_Aclzm4_rXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#^ comparing R2 scores!!!"
      ],
      "metadata": {
        "id": "-11meUdq_uFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get shared features among all voxels"
      ],
      "metadata": {
        "id": "nM3nfz7vrHCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = get_shared_features(best_features0, best_features1)\n",
        "len(x)"
      ],
      "metadata": {
        "id": "NvSMVR0oiKsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#gets the weight values of specific features\n",
        "def get_weights_of_features(weights, features):\n",
        "  feature_weights = []\n",
        "  for i in features:\n",
        "    feature_weights.append(weights[i])\n",
        "  return feature_weights"
      ],
      "metadata": {
        "id": "zm3jRkTLH7Ir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get correlation matrix"
      ],
      "metadata": {
        "id": "XHHkIRFyrLNc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#v stacks specific vox_nums and their features - can be used for all voxs too\n",
        "#just set np.arange from 0 to num_voxs\n",
        "def vstack_vox_and_features(vox_nums, features):\n",
        "  count = 0\n",
        "  arr = []\n",
        "  for num in vox_nums:\n",
        "    weights = get_weights_of_vox(coef_m, num)\n",
        "    feature_weights = get_weights_of_features(weights, features)\n",
        "    if count == 0: \n",
        "      arr.append(feature_weights)\n",
        "      count += 1\n",
        "    else:\n",
        "      arr = np.vstack((arr, feature_weights))\n",
        "  return arr"
      ],
      "metadata": {
        "id": "jBpMyQ1JRg0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = vstack_vox_and_features(well_predicted_voxs, all_shared_features)"
      ],
      "metadata": {
        "id": "B9xOZ1T1nekt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"shared highest 25% features across all 3 voxels: 484\")"
      ],
      "metadata": {
        "id": "fdREL1HY2KlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "id": "y0pSuXpsnqkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "x_corr = np.corrcoef(x)\n",
        "d = sns.heatmap(x_corr, annot=True)\n",
        "d.set_title('3 voxels & their shared features correlation')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Mfv6s4V0n9z3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#--------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "fRMRVCQPXAWM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scrap Work for Significance Stats Tests"
      ],
      "metadata": {
        "id": "BPlopypGWh38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#saving a dictionary fct\n",
        "import pickle\n",
        "\n",
        "def dict_save(dict, file_name):\n",
        "    with open(file_name + '.pickle', 'wb') as f:\n",
        "        pickle.dump(dict, f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def dict_load(dict_name):\n",
        "    with open(dict_name , 'rb') as f:\n",
        "              #+ '.pickle', 'rb') as f:\n",
        "        return pickle.load(f)"
      ],
      "metadata": {
        "id": "RdlTmrZ8xjEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###loading file by dragging file from pc to the colab workspace\n",
        "#EarlyV_coef_m_means = dict_load('EarlyV_coef_m_means.pickle')\n",
        "#pop_mean = EarlyV_coef_m_means['classifier.5']\n",
        "#pop_mean"
      ],
      "metadata": {
        "id": "ejJf1FEfaery"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###loading file w wget\n",
        "EarlyV_coef_m_means = dict_load('EarlyV_coef_m_means(2).pickle')\n",
        "EarlyV_coef_m_means.keys()\n",
        "#pop_mean = EarlyV_coef_m_means['classifier.5']\n",
        "#pop_mean"
      ],
      "metadata": {
        "id": "EzUDXtG23qF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#STATS ANALYSIS\n",
        "#to get significant features of each \n",
        "from scipy import stats\n",
        "\n",
        "#EarlyV_coef_m_means = dict_load('EarlyV_coef_m_means')\n",
        "pop_mean = EarlyV_coef_m_means['classifier.5']\n",
        "\n",
        "#values = np.arange(1000, max_vox_coef_m_sorted.size, 1000)\n",
        "#print(values)\n",
        "values = np.arange(0, 4096)\n",
        "#3 times std - 60,95,99.8 rule??? - think about stats more\n",
        "max_vox_coef_m_sorted = max_vox_coef_m1.copy()\n",
        "max_vox_coef_m_sorted = np.sort(max_vox_coef_m_sorted)\n",
        "\n",
        "print(\"POP MEAN:\", pop_mean)\n",
        "for idx in range(0, values.size-1):\n",
        "  subsample = max_vox_coef_m_sorted[values[idx]:values[idx+100]]\n",
        "  #print(\"subsample mean:\", np.mean(subsample))\n",
        "  result = stats.ttest_1samp(subsample, pop_mean, alternative='greater')\n",
        "  if (result.pvalue < .01):\n",
        "    print(\"subsample mean:\", np.mean(subsample))\n",
        "    print(\"subsample (\", values[idx],\"to\", values[idx+100],\") p-value:\", result.pvalue)\n",
        "    break\n",
        "\n",
        "#test = max_vox_coef_m_sorted[]"
      ],
      "metadata": {
        "id": "VcITA-MQczt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#visualization of it\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "\n",
        "x = range(0,50)\n",
        "y = norm.pdf(x, 0, 1)\n",
        "plt.plot(x, y)\n"
      ],
      "metadata": {
        "id": "tSAZa85uNGaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#same thing as above - but start from highest to lowest\n",
        "#to get significant features of each \n",
        "from scipy import stats\n",
        "\n",
        "#EarlyV_coef_m_means = dict_load('EarlyV_coef_m_means')\n",
        "pop_mean = EarlyV_coef_m_means['classifier.5']\n",
        "\n",
        "\n",
        "#values = [4095,4094,...,0]\n",
        "values = list(np.arange(0, 4096))\n",
        "values.reverse()\n",
        "#3 times std - 60,95,99.8 rule??? - think about stats more\n",
        "max_vox_coef_m_sorted = max_vox_coef_m1.copy()\n",
        "max_vox_coef_m_sorted = np.sort(max_vox_coef_m_sorted)\n",
        "\n",
        "print(\"POP MEAN:\", pop_mean)\n",
        "for idx in values:\n",
        "  subsample = max_vox_coef_m_sorted[idx-10:len(values)]\n",
        "  print(\"subsample mean:\", np.mean(subsample))\n",
        "  result = stats.ttest_1samp(subsample, pop_mean, alternative='greater')\n",
        "  print(\"results.pvalue:\", result.pvalue)\n",
        "  if (result.pvalue > .07):\n",
        "    print(\"subsample mean:\", np.mean(subsample))\n",
        "    #print(\"subsample (\", idx-100, \"to\", idx,\") p-value:\", result.pvalue)\n",
        "    print(\"subsample (\", idx, \"to 4096) p-value:\", result.pvalue)\n",
        "    break\n",
        "percent_features_sig = idx/4096\n",
        "print(\"percentage of useful features:\", f\"{percent_features_sig:.2%}\")\n",
        "\n",
        "#test = max_vox_coef_m_sorted[]"
      ],
      "metadata": {
        "id": "aHdBVydPoU6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3992 to 4092"
      ],
      "metadata": {
        "id": "sXlYsGbxmq26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sig_features_idxs = []\n",
        "idxs = np.arange(3992,4096)\n",
        "\n",
        "for i in idxs:\n",
        "  coef_m_value = max_vox_coef_m_sorted[i]\n",
        "  #print(\"coef_m_value:\", coef_m_value)\n",
        "  idx = np.argwhere(max_vox_coef_m1 == coef_m_value)[0][0]\n",
        "  sig_features_idxs.append(idx)\n",
        "  #print(sig_features_idxs)\n"
      ],
      "metadata": {
        "id": "KdqZcc1Tyu7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_features_subsample = best_features1[100:]\n",
        "len(best_features_subsample)\n",
        "len(sig_features_idxs)"
      ],
      "metadata": {
        "id": "fzrrdG3Fm-9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#mean of weights of each feature\n",
        "import seaborn as sns\n",
        "sns.distplot(results['COEF_M'].mean(axis=0))"
      ],
      "metadata": {
        "id": "LVw4NCnmOwKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.max(max_vox_coef_m1)"
      ],
      "metadata": {
        "id": "8l-K5q9w35xf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_vox_coef_m_copy = max_vox_coef_m1.copy()\n",
        "max_vox_coef_m_sorted = np.sort(np.abs(max_vox_coef_m_copy))\n",
        "max_vox_coef_m_sorted_top_20_percent = max_vox_coef_m_sorted[-int(max_vox_coef_m1.size*.20):]\n",
        "max_vox_coef_m_sorted_top_20_percent.size * 5\n",
        "#max_vox_coef_m_sorted_top_100\n",
        "#np.mean(max_vox_coef_m_sorted_top_100)"
      ],
      "metadata": {
        "id": "Fq-qnYVf6o7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EarlyV_coef_m_means = dict_load('EarlyV_coef_m_means(2).pickle')"
      ],
      "metadata": {
        "id": "BZBFbCfd_rFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pop_mean = EarlyV_coef_m_means['features.3']\n",
        "pop_mean\n",
        "# = 2.2886144813910386e-05"
      ],
      "metadata": {
        "id": "qyfCZAh14Hq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install --upgrade scipy"
      ],
      "metadata": {
        "id": "C7JThArv8oWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#--------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "QyC18PvNcvRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting Mean of all Weights from each Layer"
      ],
      "metadata": {
        "id": "4YGelAMOn8Ee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#saving a dictionary fct\n",
        "import pickle\n",
        "\n",
        "def dict_save(dict, file_name):\n",
        "    with open(file_name + '.pickle', 'wb') as f:\n",
        "        pickle.dump(dict, f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def dict_load(dict_name):\n",
        "    with open(dict_name + '.pickle', 'rb') as f:\n",
        "        return pickle.load(f)"
      ],
      "metadata": {
        "id": "JkRqQufHoKLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EarlyV_coef_m_means = dict_load('EarlyV_coef_m_means(2)')\n",
        "EarlyV_coef_m_means.keys()"
      ],
      "metadata": {
        "id": "6y_k9_qXHqCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get coef_m_mean of a single sub\n",
        "def get_coef_m_mean(sub, region, layer_name):\n",
        "  sub_betas = betas[region][sub].transpose()\n",
        "  pred_rdms, feat_rdms, results = fit_encoding_model(sub_betas,\n",
        "                                                   #change layer name\n",
        "                                                   layer_name = layer_name,\n",
        "                                                   model_name='alexnet',\n",
        "                                                   dataset='InanimateObjects', \n",
        "                                                   mean=[0.485, 0.456, 0.406], \n",
        "                                                   std=[0.229, 0.224, 0.225])\n",
        "  #gets_coef_m from restults - takes abs value - includes neg weights too\n",
        "  coef_m_mean = np.mean(np.abs(results['COEF_M']))\n",
        "  return coef_m_mean\n",
        "\n",
        "#get the average of all coef_ms for a single region\n",
        "def get_mean_of_all_coef_ms(layer_name, region):\n",
        "  subs = [0,1,2,3,4,5,6,7,8,9]\n",
        "  coef_m_means = []\n",
        "  for sub in subs:\n",
        "     print(\"\\nSUB:\", sub)\n",
        "     #gets coef_m_mean of a single sub\n",
        "     coef_m_mean = get_coef_m_mean(sub, region, layer_name)\n",
        "     print(\"mean of sub\", sub, \":\", coef_m_mean)\n",
        "     #appends to the running list\n",
        "     coef_m_means.append(coef_m_mean)\n",
        "     #prints the current mean of the list\n",
        "     current_mean = np.mean(coef_m_means)\n",
        "     #current_mean = (np.sum(coef_m_means) / len(coef_m_means))\n",
        "     print(\"overall mean at sub\", sub, \":\", current_mean)\n",
        "  print(\"FINAL MEAN:\", current_mean)   \n",
        "  mean_of_all_coef_ms = np.sum(coef_m_means) / len(coef_m_means)\n",
        "  return coef_m_means, mean_of_all_coef_ms"
      ],
      "metadata": {
        "id": "BNeFqDljqJSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Taking mean of layers"
      ],
      "metadata": {
        "id": "D3wxw41aXSRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_layers(model, parent_name='', layer_info=[]):\n",
        "    for module_name, module in model.named_children():\n",
        "        layer_name = parent_name + '.' + module_name\n",
        "        if len(list(module.named_children())):\n",
        "            layer_info = get_layers(module, layer_name, layer_info=layer_info)\n",
        "        else:\n",
        "            layer_info.append(layer_name.strip('.'))\n",
        "    \n",
        "    return layer_info\n",
        "def get_layer_names(model):\n",
        "    return get_layers(model, parent_name='', layer_info=[])"
      ],
      "metadata": {
        "id": "IhYyfjS0QDjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.alexnet(pretrained=True)\n",
        "layer_names = get_layer_names(model)\n",
        "#layer_names = layer_names[:5] \n",
        "layer_names"
      ],
      "metadata": {
        "id": "CENvjFKDKtO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#gives layers still needed to run\n",
        "x = layer_names\n",
        "y = list(EarlyV_coef_m_means.keys())\n",
        "layers_needed_to_run = []\n",
        "for i in x:\n",
        "  if i not in y:\n",
        "    layers_needed_to_run.append(i)\n",
        "\n",
        "layers_needed_to_run"
      ],
      "metadata": {
        "id": "nMWrsjFVUQQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.alexnet(pretrained=True)\n",
        "layer_names = get_layer_names(model)\n",
        "layer_names = layer_names[4] \n",
        "layer_names"
      ],
      "metadata": {
        "id": "oMCvZg5yWXKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.alexnet(pretrained=True)\n",
        "\n",
        "layer_names = get_layer_names(model)\n",
        "layer_names = layer_names[4]  \n",
        "print(\"layer_names:\", layer_names)\n",
        "\n",
        "#coef_m_means for all layers in EarlyV\n",
        "#EarlyV_coef_m_means = {}\n",
        "#EarlyV_coef_m_means = dict_load('EarlyV_coef_m_means')\n",
        "region = 'EarlyV'\n",
        "print('\\n\\n\\nBRAIN REGION:', region)\n",
        "for layer_name in layer_names:\n",
        "  print('\\n\\n\\nLAYER NAME:', layer_name)\n",
        "  layer_name = ['features.4']\n",
        "  coef_m_means, mean_of_all_coef_ms = get_mean_of_all_coef_ms(region = region, layer_name=layer_name)\n",
        "\n",
        "  EarlyV_coef_m_means[layer_name] = mean_of_all_coef_ms\n",
        "\n",
        "#save the dict\n",
        "dict_save(EarlyV_coef_m_means, 'EarlyV_coef_m_means')"
      ],
      "metadata": {
        "id": "Vu3Jxswk4bPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for a single one:\n",
        "model = models.alexnet(pretrained=True)\n",
        "\n",
        "layer_name = 'features.4'\n",
        "print(\"layer_name:\", layer_name)\n",
        "\n",
        "region = 'EarlyV'\n",
        "print('\\n\\n\\nBRAIN REGION:', region)\n",
        "\n",
        "print('\\n\\n\\nLAYER NAME:', layer_name)\n",
        "\n",
        "coef_m_means, mean_of_all_coef_ms = get_mean_of_all_coef_ms(region = region, layer_name=layer_name)\n",
        "EarlyV_coef_m_means[layer_name] = mean_of_all_coef_ms\n",
        "\n",
        "\n",
        "dict_save(EarlyV_coef_m_means, 'EarlyV_coef_m_means')\n"
      ],
      "metadata": {
        "id": "jbe6FxVydQwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EarlyV_coef_m_means.keys()"
      ],
      "metadata": {
        "id": "CIoPyGmLZYiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_save(EarlyV_coef_m_means, 'EarlyV_coef_m_means')"
      ],
      "metadata": {
        "id": "BAwjA392FlUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('EarlyV_coef_m_means.pickle')"
      ],
      "metadata": {
        "id": "Y43yxaUIRQrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = dict_load('EarlyV_coef_m_means')\n",
        "x"
      ],
      "metadata": {
        "id": "At00RPNAge31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EarlyV_coef_m_means.keys()"
      ],
      "metadata": {
        "id": "HTlFa_fMo_jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model\n",
        "#from google.colab import files\n",
        "#files.download('EarlyV_coef_m_means')"
      ],
      "metadata": {
        "id": "SdZzo8GEjX2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_save(EarlyV_coef_m_means, 'EarlyV_coef_m_means')"
      ],
      "metadata": {
        "id": "7UmjHQvPQVKW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}